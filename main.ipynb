{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1r0ja+ps4GN6xGzHhLESD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/negarslh97/AI_assistant/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaDp8RKMQYkt",
        "outputId": "0b549884-56b2-489b-eccf-e840d309cede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "from faiss import IndexFlatL2"
      ],
      "metadata": {
        "id": "-H2Q5XBwRcgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae4ac89-f6a5-49ad-b97e-f4a42ca14412"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Layer"
      ],
      "metadata": {
        "id": "N1-dqz9tTnBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionProcessor:\n",
        "  def preprocess(self, question):\n",
        "      # Perform normalization, lemmatization, and remove stop words\n",
        "        processed_question = question.lower().strip()\n",
        "        return processed_question\n",
        ""
      ],
      "metadata": {
        "id": "Fwr6aHabRci3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FAQMatcher:\n",
        "  def __init__(self, faq_data, threshold=0.75):\n",
        "    self.faq_data = faq_data\n",
        "    self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "    self.threshold = threshold\n",
        "    self.faq_embeddings = self._create_embeddings([item['question'] for item in faq_data])\n",
        "\n",
        "  def _create_embeddings(self, texts):\n",
        "    # Create embeddings for a list of texts\n",
        "    return self.model.encode(texts)\n",
        "\n",
        "  def find_match(self, question):\n",
        "    # Encode the input question\n",
        "    question_embedding = self.model.encode([question])\n",
        "\n",
        "    # Compute similarities with FAQ questions\n",
        "    similarities = cosine_similarity(question_embedding, self.faq_embeddings)[0]\n",
        "\n",
        "    # Find the best match\n",
        "    max_similarity_idx = similarities.argmax()\n",
        "    max_similarity = similarities[max_similarity_idx]\n",
        "\n",
        "    if max_similarity >= self.threshold:\n",
        "        return self.faq_data[max_similarity_idx]['answer']\n",
        "    return None\n",
        "\n",
        "\n",
        "  def optimize_threshold(self, questions, labels):\n",
        "    # Optimize threshold using validation data\n",
        "    best_threshold = 0\n",
        "    best_accuracy = 0\n",
        "    for t in [i/100 for i in range(50, 100)]:\n",
        "        accuracy = self._calculate_accuracy(questions, labels, t)\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = t\n",
        "    self.threshold = best_threshold\n",
        "\n",
        "  def _calculate_accuracy(self, questions, labels, threshold):\n",
        "    # Calculate accuracy for a given threshold\n",
        "    correct = 0\n",
        "    for question, label in zip(questions, labels):\n",
        "        prediction = self.find_match(question)\n",
        "        if (prediction is not None) == label:\n",
        "            correct += 1\n",
        "    return correct / len(labels)"
      ],
      "metadata": {
        "id": "6C8TqNe2UV-C"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeRetriever:\n",
        "  def __init__(self, context_data):\n",
        "    self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "    self.context_data = context_data\n",
        "    self.context_embeddings = self.model.encode([item['text'] for item in context_data])\n",
        "    self.index = IndexFlatL2(self.context_embeddings.shape[1])\n",
        "    self.index.add(self.context_embeddings)\n",
        "\n",
        "  def retrieve_context(self, query):\n",
        "    query_embedding = self.model.encode([query])\n",
        "    distances, indices = self.index.search(query_embedding, k=5)\n",
        "    results = [self.context_data[i]['text'] for i in indices[0]]\n",
        "    return \" \".join(results)"
      ],
      "metadata": {
        "id": "vF-5RG34q0zP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTResponder:\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  def generate_response(self, question, context=None):\n",
        "    # Call to GPT model\n",
        "    return self.model.generate_response(question, context)"
      ],
      "metadata": {
        "id": "4xlnioPtUZPr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factory Layer"
      ],
      "metadata": {
        "id": "uI3g_d4sUcfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Factory Pattern\n",
        "class ResponseFactory:\n",
        "  def __init__(self, faq_matcher, gpt_responder, knowledge_retriever):\n",
        "    self.faq_matcher = faq_matcher\n",
        "    self.gpt_responder = gpt_responder\n",
        "    self.knowledge_retriever = knowledge_retriever\n",
        "\n",
        "  def get_response(self, question):\n",
        "    response = self.faq_matcher.find_match(question)\n",
        "    if response:\n",
        "        return response\n",
        "\n",
        "    retrieved_context = knowledge_retriever.retrieve_context(question)\n",
        "    return self.gpt_responder.generate_response(question, context=retrieved_context)\n",
        "\n",
        "    # Retrieve additional context\n",
        "    context = self.knowledge_retriever.retrieve_context(question)\n",
        "\n",
        "    # FallBack to GPT response with context\n",
        "    return self.gpt_responder.generate_response(question, context)"
      ],
      "metadata": {
        "id": "a-VuReH2Rcln"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application Layer"
      ],
      "metadata": {
        "id": "ELsO2EprUjag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Assistant:\n",
        "  def __init__(self, processor, factory):\n",
        "    self.processor = processor\n",
        "    self.factory = factory\n",
        "\n",
        "  def handle_question(self, question):\n",
        "    processed_question = self.processor.preprocess(question)\n",
        "    response = self.factory.get_response(processed_question)\n",
        "    return response"
      ],
      "metadata": {
        "id": "6eY8IkmPRcoP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage"
      ],
      "metadata": {
        "id": "HBBZMU9BUppJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  faq_file_path = \"/content/drive/My Drive/SadGan/Preprocessed_Amazon_sagemaker_Faq.json\"\n",
        "  with open(faq_file_path, 'r') as file:\n",
        "      faq_data = json.load(file)\n",
        "\n",
        "  # Create mock GPT model\n",
        "  class MockGPTModel:\n",
        "      def generate_response(self, question, context=None):\n",
        "          return f\"GPT Response for: {question} with context: {context}\"\n",
        "\n",
        "  # Create components\n",
        "  question_processor = QuestionProcessor()\n",
        "  faq_matcher = FAQMatcher(faq_data)\n",
        "  gpt_responder = GPTResponder(MockGPTModel())\n",
        "\n",
        "  # Create context data\n",
        "  context_data = [\n",
        "      {\"text\": \"Amazon SageMaker is a cloud machine learning platform.\"},\n",
        "      {\"text\": \"It provides developers with the ability to build, train, and deploy ML models.\"}\n",
        "  ]\n",
        "  knowledge_retriever = KnowledgeRetriever(context_data)\n",
        "\n",
        "  # Create response factory\n",
        "  response_factory = ResponseFactory(faq_matcher, gpt_responder, knowledge_retriever)\n",
        "  assistant = Assistant(question_processor, response_factory)"
      ],
      "metadata": {
        "id": "St7nG5r3Rcqt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "Wy7f5kXpUraf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = input(\"Enter your question (or type 'exit' to quit): \").strip()\n",
        "if user_question.lower() == 'exit':\n",
        "  print(\"Goodbye!\")\n",
        "\n",
        "response = assistant.handle_question(user_question)\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzs7TGzxSAFT",
        "outputId": "4b67ab50-7976-4a9f-d4d3-6818e9aa3106"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (or type 'exit' to quit): in which regions is amazon sagemaker available\n",
            "Response: For a list of the supported Amazon SageMaker AWS regions, please visit the AWS Region Table for all AWS global infrastructure. Also for more information, see Regions and Endpoints in the AWS General Reference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Response flow test\n",
        "user_question = \"What can Amazon SageMaker do?\"\n",
        "response = response_factory.get_response(user_question)\n",
        "print(f\"User Question: {user_question}\")\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPPc9r4BzTPx",
        "outputId": "52059ff6-c10d-4507-e5b5-572cbc44d9f8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Question: What can Amazon SageMaker do?\n",
            "Response: Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.\n"
          ]
        }
      ]
    }
  ]
}